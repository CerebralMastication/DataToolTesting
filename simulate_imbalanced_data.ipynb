{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25f94438-5aaf-473d-8ca2-53b354d43856",
   "metadata": {},
   "source": [
    "# Simulate Fake Data\n",
    "\n",
    "This chunk of code was created for a different project so there's a few hoops in here that are there totally to make the cardinality highly skewed. Those don't matter here, but note that the final data set has parquet files that vary wildly in size. \n",
    "\n",
    "To run this you will need the following path in your working directory: `./output/df1/` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17b714c-584f-45a2-87d1-24787c19e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f21b26e-2733-4ea4-b2ac-93246bbaee88",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "These are constants used below, they are described in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86632d90-7167-4ae1-a376-35b3a710671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 constants:\n",
    "\n",
    "bucket_n =  40_000   \n",
    "key_n    =  7000  \n",
    "group1_n = 10\n",
    "group2_n = 20\n",
    "group3_n = 30\n",
    "group4_n = 40\n",
    "\n",
    "P_maj1 = (160, 125125)     # parameters of the majority exp dist\n",
    "P_min1 = (1006402, 8200726) # parameters of the minority exp dist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9f02a9-3ab1-4cda-bd64-5cd5a234726b",
   "metadata": {},
   "source": [
    "## `df1` - data with large record count\n",
    "\n",
    "The first dataframe is going to be  `df1` and it is the larger table in terms of record count. The fields in `df1` are as follows:\n",
    "\n",
    "`key`: the key. The number of unique keys is low compared to the number of records in each key\n",
    "\n",
    "`bucket1`: Every key has the same number of unique values in `bucket1`. That number is set by the constant `bucket_n` above\n",
    "\n",
    "`group1`: There are 4 groupings. Each of these groupings is assinged a key and they are later used in a group by \n",
    "\n",
    "`group2`: see above\n",
    "\n",
    "`group3`: see above\n",
    "\n",
    "`group4`: see above\n",
    "\n",
    "`value`: This is the value we will sum later. It's random normal. \n",
    "\n",
    "\n",
    "## Simulate `df1`\n",
    "\n",
    "Let's build `df1`:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8f4f91-1525-42aa-8e63-2c5e8c96ebd6",
   "metadata": {},
   "source": [
    "Using the original problematic data, the number of records per bucket is distributed like a double exponential distribution (i.e. two exponential distributions mixed together). 95% of the buckets get their draw from a somewhat shorter tailed distribution, and 5% get their number of record draws from a much longer tail distribution.\n",
    "\n",
    "This will give us roughly half a million sims per key if we use the exponential parameters set up above (`P_maj1` & `P_min1` ) and 40,000 buckets per key.\n",
    "\n",
    "95% first:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de72c91-701d-470f-9338-5603da501de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "draws_per_key_majority = st.expon.rvs(*P_maj1, size= round(.95 * key_n)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142e30a7-1a52-45b5-87f9-1a5bc172f3e4",
   "metadata": {},
   "source": [
    "then the 5%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7624d6e7-debc-40d6-b541-5c33233f354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "draws_per_key_minority = st.expon.rvs(*P_min1,  size= round(.05 * key_n)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136cbb80-eb33-4e9a-8c2a-45a6714cc0f8",
   "metadata": {},
   "source": [
    "note that doing fractions from one dist then another fraction from another can end up with an off by one error. We might have 10 keys but only end up simulating 9. The probability of this happening goes down as number of keys goes up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807753f1-3fd9-4da8-b878-fda0c86b950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "draws_per_key = np.concatenate((draws_per_key_majority, draws_per_key_minority))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd09378-ad3c-4a24-88c0-4f3f3346ace1",
   "metadata": {},
   "source": [
    "Total number of records that will be in `df1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a0c2db-7a2e-4610-a4ca-55c05017d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(draws_per_key) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564e98b6-a9de-4767-989b-afc6cd27fc7f",
   "metadata": {},
   "source": [
    "So now we know how many buckets and how many records per bucket. So the simulation of `df1` will be to loop over the `draws_per_bucket` and draw that many observations with random groups. This could all be vectorized but I'm keeping this a loop to keep it readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ef865d-32b4-451d-8c73-87b44bfba2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## 33 min on my MBP and generates ~52GB of parquet files\n",
    "\n",
    "df1_list = []\n",
    "key = 1\n",
    "\n",
    "# simulate values for each bucket. Simulate a df with a single value for the key then randomly assign groups and values\n",
    "\n",
    "for draws in draws_per_key:\n",
    "    df = pd.DataFrame()\n",
    " \n",
    "    df[\"key\"] = np.resize(key, draws)\n",
    "    \n",
    "    df[\"bucket\"] = np.resize(np.arange(1, bucket_n + 1), draws)\n",
    "\n",
    "    df[\"group1\"] = np.random.randint(low=1, high=group1_n, size=draws)\n",
    "    df[\"group2\"] = np.random.randint(low=1, high=group2_n, size=draws)\n",
    "    df[\"group3\"] = np.random.randint(low=1, high=group3_n, size=draws)\n",
    "    df[\"group4\"] = np.random.randint(low=1, high=group4_n, size=draws)\n",
    "    df[\"value\"] = np.random.random(size=draws)\n",
    "    \n",
    "    #df1_list.append(df)\n",
    "    df.to_parquet(f'./output/df1/key_{str(key).zfill(5)}.parquet')\n",
    "    \n",
    "    key = key + 1\n",
    "    \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9538ba2-8134-4aa8-a8b1-2d4ddebdd0fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e91822-630b-4e00-a065-f9301e2ce0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10904a9-66e1-4f5a-9e02-659cd6237669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f3bd2d-b5b0-4dff-9482-ffaee5c93f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_stuff",
   "language": "python",
   "name": "data_stuff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
